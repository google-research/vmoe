# From Sparse to Soft Mixture of Experts

This folder contains the implementation of Soft MoE, presented in the paper:

- [From Sparse to Soft Mixtures of Experts](https://arxiv.org/abs/2308.00951),
  by Joan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil Houlsby.

We provide the config files used to run some of the experiments reported in the
paper.

Notice that all experiments either train on JFT-4B, a proprietary dataset,
or use models pre-trained on it, thus we cannot release any of the checkpoints.
